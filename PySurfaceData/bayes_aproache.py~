#!/usr/bin/env python

import matplotlib.pyplot as plt
import math
import numpy as np
import torch
from torch import nn
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader
from datetime import datetime, timedelta
import pandas as pd
import os
import scipy
from scipy import stats
import time
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import tempfile
import torch.distributed as dist
import sys

import warnings

from ModelOnePointFour import *

from multiprocessing.pool import Pool
    

def save_data_in_paralelle(elements,N=2000,learning_rate = 1e-3,initial_conditions_path = '/Users/carlos/Documents/surface_data_analisis/npy_data/_0'):
    '''
    use sorted elements from oldest to newest. 
    '''
    data_i = elements[0]
    date_i = elements[1]
    output_path = elements[2]
    x_a = elements[3]
    s_a = elements[4]
    s_e = elements[5]
    constant = elements[6]

    num_days = len(date_i)
    
    Loss = custom_Loss(x_a,s_a,s_e,num_days = num_days)
    F_model = Forward_Model(num_days = num_days).to("cpu")

    initial_conditions = torch.empty((num_days,1,3))
    for i in range(num_days):
        initial_conditions[i,0] = torch.tensor(np.load(initial_conditions_path + '/'+ str(int(date_i[i]))+'.npy'))
        
    initial_conditions[initial_conditions == 0] = torch.rand(1)
        
    state_dict = F_model.state_dict()
    state_dict['chparam'] = torch.ones((num_days,1,3), dtype=torch.float32)*initial_conditions
    F_model.load_state_dict(state_dict)
    optimizer = torch.optim.Adam(F_model.parameters(),lr=learning_rate)
    dates = [datetime(year = 2000,month=1,day=1) + timedelta(days=int(date))  for date in date_i]
    
    output_name = dates[0].strftime("%Y%m%d") + '-' + dates[-1].strftime("%Y%m%d")
    if os.path.exists(output_path + '/' + output_name ):
            pass
    else:
            os.mkdir( output_path + '/' + output_name)

            
    output = train_loop(data_i,F_model,Loss,optimizer,2000,kind='all',num_days = num_days,constant = constant)
    #print(output['X_hat'][0])
    
    np.save(output_path + '/' + output_name + '/X_hat.npy',output['X_hat'] )
    np.save(output_path + '/' + output_name + '/kd_hat.npy',output['kd_hat'])
    np.save(output_path + '/' + output_name + '/bbp_hat.npy',output['bbp_hat'] )
    np.save(output_path + '/' + output_name + '/RRS_hat.npy',output['RRS_hat'] )
    np.save(output_path + '/' + output_name + '/dates.npy',date_i)
    
if __name__ == '__main__':

    initial_conditions_path = '/Users/carlos/Documents/surface_data_analisis/npy_data/'
    data_path = '/Users/carlos/Documents/surface_data_analisis/npy_data/'
    data = customTensorData(data_path='./npy_data',which='all')

    #mps_device = torch.device("mps")
    my_device = 'cpu'

    
    constant = read_constants(file1='cte_lambda.csv',file2='cst.csv',my_device = my_device)

    dataloader = DataLoader(data, batch_size=1, shuffle=False)

    x_a = torch.ones(3)
    s_a = torch.eye(3)*10
    output_file = '/Users/carlos/Documents/surface_data_analisis/results_bayes_optimized'
    s_e = (torch.eye(5)*torch.tensor([1.5e-3,1.2e-3,1e-3,8.6e-4,5.7e-4]))**2#validation rmse from https://catalogue.marine.copernicus.eu/documents/QUID/CMEMS-OC-QUID-009-141to144-151to154.pdf

    batch_size = 1937
    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)

    for learning_rate in [1e-3]:
        
        save_data_in_paralelle(   (next(iter(dataloader)), data.dates , output_file, x_a, s_a, s_e, constant ),learning_rate = learning_rate,N=2000)
    
    #with Pool() as pool:       
    #    pool.map(save_data_in_paralelle ,[  ([[data.__getitem__(index)[0]],[data.__getitem__(index)[1]]],data.dates[index], output_file, x_a, s_a, s_e ) for index in data.my_indexes ])


  
