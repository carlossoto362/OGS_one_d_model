#!/usr/bin/env python

import matplotlib.pyplot as plt
import math
import numpy as np
import torch
from torch import nn
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader
from datetime import datetime, timedelta
import pandas as pd
import os
import scipy
from scipy import stats
import time
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import tempfile
import torch.distributed as dist
import sys

from ModelOnePointFour import *

import warnings



def data_dataframe(data_path,which='all'):
    data = customTensorData(data_path=data_path,which=which,per_day = False,randomice=False)
    
    dataframe = pd.DataFrame(columns = data.x_column_names + data.y_column_names + data.init_column_names)
    print(data.init_column_names,data.init.shape)
    dataframe[data.x_column_names] = data.x_data
    dataframe[data.y_column_names] = data.y_data
    dataframe[data.init_column_names] = data.init
    dataframe['date'] = [datetime(year=2000,month=1,day=1) + timedelta(days=date) for date in data.dates]
    dataframe.sort_values(by='date',inplace=True)
    return dataframe
    


def read_second_run(second_run_path,include_uncertainty=False,abr='output',name_index = None, ignore_name = None):
    if include_uncertainty == False:
        second_run_output = pd.DataFrame(columns=['RRS_'+abr+'_412','RRS_'+abr+'_442','RRS_'+abr+'_490','RRS_'+abr+'_510','RRS_'+abr+'_555',\
                                                  'chla_'+abr,'NAP_'+abr,'CDOM_'+abr,\
                                                  'kd_'+abr+'_412','kd_'+abr+'_442','kd_'+abr+'_490','kd_'+abr+'_510',\
                                                  'kd_'+abr+'_555','bbp_'+abr+'_442','bbp_'+abr+'_490','bbp_'+abr+'_555'])
        len_kd = 5
        len_bbp = 3
        len_chla = 3
    else:

        second_run_output = pd.DataFrame(columns=['RRS_'+abr+'_412','RRS_'+abr+'_442','RRS_'+abr+'_490','RRS_'+abr+'_510','RRS_'+abr+'_555',\
                                                  'chla_'+abr, 'delta_chla_'+abr,'NAP_'+abr,'delta_NAP_'+abr,'CDOM_'+abr,'delta_CDOM_'+abr,\
                                                  'kd_'+abr+'_412','delta_kd_'+abr+'_412','kd_'+abr+'_442','delta_kd_'+abr+'_442','kd_'+abr+'_490','delta_kd_'+abr+'_490','kd_'+abr+'_510','delta_kd_'+abr+'_510',\
                                                  'kd_'+abr+'_555','delta_kd_'+abr+'_555','bbp_'+abr+'_442','delta_bbp_'+abr+'_442','bbp_'+abr+'_490','delta_bbp_'+abr+'_490','bbp_'+abr+'_555','delta_bbp_'+abr+'_555'])
        len_kd = 10
        len_bbp = 6
        len_chla = 6

    if name_index == None:
        RRS_name = 'RRS_hat.npy'
        X_name = 'X_hat.npy'
        kd_name = 'kd_hat.npy'
        bbp_name = 'bbp_hat.npy'
    else:
        RRS_name = 'RRS_hat'+'_'+name_index+'.npy'
        X_name = 'X_hat'+'_'+name_index+'.npy'
        kd_name = 'kd_hat'+'_'+name_index+'.npy'
        bbp_name = 'bbp_hat'+'_'+name_index+'.npy'

    second_run_output[second_run_output.columns[:5]] = np.load(second_run_path + '/'+RRS_name)
    second_run_output[second_run_output.columns[5:5+len_chla]] = np.load(second_run_path + '/'+X_name)
    second_run_output[second_run_output.columns[5+len_chla:5+len_chla + len_kd]] = np.load(second_run_path + '/'+kd_name)
    second_run_output[second_run_output.columns[5+len_chla + len_kd:]] = np.load(second_run_path + '/'+bbp_name)
    dates = np.load(second_run_path + '/dates.npy')
    second_run_output['date'] = [datetime(year=2000,month=1,day=1) + timedelta(days=date) for date in dates]
    second_run_output.sort_values(by='date',inplace=True)

    return second_run_output

#def read_NN_run():


def plot_sequential(data,columns,names,labels,statistics = True,histogram=True,figsize=(18,9),date_init = None, date_end = None, shadow_error = False):
    
    if date_init != None:
        data_use = data[data['date'] >= date_init]
    else:
        data_use = data
    if date_end != None:
        data_use = data_use[data_use['date'] <= date_end]
        
    dates = data_use['date']
    num_plots = len(columns)

    
    if histogram == True:
        fig,axs = plt.subplots(num_plots,2,width_ratios = [2.5,1],figsize = figsize)
    else:
        fig,axs = plt.subplots(num_plots,1,figsize = figsize)
    
    for i,column in enumerate(columns):

        
        if statistics == True:

            data_statistics_1 = data_use[column[0]][~data_use[column[0]].isnull()]
            data_statistics_2 = data_use[column[1]][~data_use[column[0]].isnull()]
            ks = stats.kstest(data_statistics_1,data_statistics_2)
            corr = stats.pearsonr(data_statistics_1,data_statistics_2)
            rmse = np.sqrt(np.mean((data_statistics_1-data_statistics_2)**2))
            bias = np.mean((data_statistics_1-data_statistics_2))
            statistic_text = "KS Test: {:.3f}, p-value: {:.3E}\nRMSE: {:.3E}\nCorrelation: {:.3f}\nBias: {:.3E}".format(ks.statistic,ks.pvalue,rmse,corr.statistic,bias)

            



def plot_parallel(data,columns,names,labels,statistics = True,histogram=True,figsize=(30,17),date_init = None, date_end = None, shadow_error = False,num_cols = 2,figname = 'fig.png',fontsize=15,colors = 1,save=True):
    if colors == 1:
        colors_palet = ['#377eb8','blue']
    elif colors == 2:
        colors_palet = ['#FFC107','#D81B60']
    elif colors == 3:
        colors_palet = ['#15CAAB','#247539']
    
    if date_init != None:
        data_use = data[data['date'] >= date_init]
    else:
        data_use = data
    if date_end != None:
        data_use = data_use[data_use['date'] <= date_end]
        
    dates = data_use['date']
    num_plots = math.ceil(len(columns)/num_cols)

    
    if histogram == True:
        fig,axs = plt.subplots(num_plots,num_cols*2,width_ratios = [2.5/num_cols,1/num_cols]*num_cols,figsize = figsize,tight_layout=True)
    else:
        fig,axs = plt.subplots(num_plots,num_cols,figsize = figsize,tight_layout=True)

    k = 0
    for j in range(num_cols):
        if j == num_cols - 1:
            end_enum = (len(columns) - num_plots*(num_cols - 1) ) + (num_plots)*j

            if histogram == True:
                for ax in axs[(len(columns) - num_plots*(num_cols - 1) ):]:
                    ax[-2].axis('off')
                    ax[-1].axis('off')
            else:
                for ax in axs[(len(columns) - num_plots*(num_cols - 1) ):,-1]:
                    ax.axis('off')
        else:
            end_enum = (num_plots)*(j+1)
        for i,column in enumerate(columns[j*(num_plots):end_enum]):
            if statistics == True:
                if pd.isnull(data_use[column[0]]).all():
                    statistic_text = ''
                else:
                    data_statistics_1 = data_use[column[0]][~data_use[column[0]].isnull()]
                    data_statistics_2 = data_use[column[1]][~data_use[column[0]].isnull()]

                    ks = stats.kstest(data_statistics_1,data_statistics_2)
                    corr = stats.pearsonr(data_statistics_1,data_statistics_2)
                    rmse = np.sqrt(np.mean((data_statistics_1-data_statistics_2)**2))
                    bias = np.mean((data_statistics_1-data_statistics_2))
                    statistic_text = "KS Test: {:.3f}, p-value: {:.3E}\nRMSE: {:.3E}\nCorrelation: {:.3f}\nBias: {:.3E}".format(ks.statistic,ks.pvalue,rmse,corr.statistic,bias)

            
            if histogram == True:
                ax = axs[i,2*j]
            elif num_cols == 1:
                ax = axs[i]
                
            else:
                ax = axs[i,j]

            if len(column) >= 1:
                if pd.isnull(data_use[column[0]]).all():
                    pass
                else:
                    ax.scatter(dates,data_use[column[0]],marker = 'o',label=labels[k][0],color='black',s=2,zorder = 10,alpha=0.7)
                ax.set_xlabel('date',fontsize=fontsize)
                ax.set_ylabel(names[k],fontsize=fontsize)
                if (statistics == True) and (histogram == False):
                    ax.text(0.99, 0.95, statistic_text, transform=ax.transAxes, fontsize=fontsize*0.7, verticalalignment='top',horizontalalignment = 'right',bbox = dict(boxstyle='round', facecolor='white', alpha=0.8,edgecolor='white'),zorder=20)
            
            if len(column) == 2:
                ax.scatter(dates,data_use[column[1]],marker ='*',label=labels[k][1],color=colors_palet[0],alpha=0.6,s=2,zorder = 5)
            if len(column) == 3:
                data_plot = pd.DataFrame({'error_up':data_use[column[2]].abs(),'error_down':data_use[column[2]].abs(),'data':data_use[column[1]]})
                data_plot['error_down'] = data_plot[['error_down','data']].min(axis=1)
                if shadow_error == False:
                    ax.errorbar(dates, data_plot['data'], yerr=[data_plot['error_down'],data_plot['error_up']], capsize=2, fmt="o", c=colors_palet[1],ecolor=colors_palet[0],ms=2,alpha=0.6,zorder=5,label = labels[k][1])
                else:
                    ax.scatter(dates,data_use[column[1]],marker ='*',label=labels[k][1],color=colors_palet[1],alpha=0.6,s=2,zorder = 5)
                    ax.fill_between(dates, data_plot['data']-data_plot['error_down'], data_plot['data']+data_plot['error_up'],color=colors_palet[0],alpha=0.6,zorder = 1)
            ax.legend(loc='upper left')
            ax.tick_params(axis='x', labelsize=fontsize)
            ax.tick_params(axis='y', labelsize=fontsize)
        
            if histogram == True:
                ax = axs[i,2*j + 1]
                if len(column) >= 1:
                    data_plot = data_use[column[0]][~data_use[column[0]].isnull()]
                    ax.hist(data_plot,bins=20,label=labels[k][2],density=True,color='black',edgecolor='gray')
                    ax.set_xlabel(names[k],fontsize=fontsize)
                    #ax.set_xlim(0,0.3)
                    if (statistics == True):
                        ax.text(0.99, 0.7, statistic_text, transform=ax.transAxes, fontsize=fontsize*0.7, verticalalignment='top',horizontalalignment = 'right',bbox = dict(boxstyle='round', facecolor='white', alpha=0.8,edgecolor='white'),zorder = 20)
                    if len(column) >= 2:
                        data_plot = data_use[column[1]][~data_use[column[0]].isnull()]
                        ax.hist(data_plot,bins=20,label=labels[k][3],alpha=0.5,density=True,color=colors_palet[0],edgecolor=colors_palet[1])
                ax.legend(loc='upper right')
                ax.tick_params(axis='x', labelsize=fontsize)
                ax.tick_params(axis='y', labelsize=fontsize)
            k+=1
    plt.tight_layout()
    if save == True:
        plt.savefig(figname)
    else:
        plt.show()


def plot_with_u():

    data = data_dataframe('/Users/carlos/Documents/surface_data_analisis/LastVersion/npy_data')

    data['NAP'] = np.nan
    data['CDOM'] = np.nan
    second_run = read_second_run('/Users/carlos/Documents/surface_data_analisis/LastVersion/results_bayes_optimized',include_uncertainty=True,abr='output')
    data = second_run.merge(data,how='right',on='date')
    data.sort_values(by='date',inplace=True)
    del second_run

    
    lambdas_names = ['412','442','490','510','555']
    lambdas_values = ['412.5','442.5','490','510','555']

    columns = []
    names = []
    labels = []

    for i,lam in enumerate(lambdas_names):
        columns.append(('kd_'+ lam,'kd_output_' + lam,'delta_kd_output_'+lam))
        names.append('$kd_{'+lambdas_values[i]+'} (m^{-1}$)')
        labels.append(('Buoy data','Bayes aproach','Buoy data','Bayes aproach'))




    columns.append(('chla','chla_output','delta_chla_output'))
    names.append('$Chl-a (mg/m^{3})$')
    labels.append(('Buoy data','Bayes aproach', 'Buoy data', 'Bayes aproach'))
    for i,lam in enumerate(lambdas_names):
        if (i == 1) or (i == 2) or (i ==4):
            columns.append(('bbp_'+ lam,'bbp_output_' + lam,'delta_bbp_output_'+lam))
            names.append('$b_{b,p,'+lambdas_values[i]+'} (m^{-1}$)')
            labels.append(('Buoy data','Bayes aproach','Buoy data','Bayes aproach'))

    columns.append(('NAP','NAP_output','delta_chla_output'))
    names.append('$NAP (mg/m^3)$')
    labels.append(('','Bayes aproach','','Bayes aproach'))

    columns.append(('CDOM','CDOM_output','delta_CDOM_output'))
    names.append('$CDOM (mg/m^3)$')
    labels.append(('','Bayes aproach','','Bayes aproach'))


    plot_parallel(data,columns,names,labels,statistics = True,histogram=False,date_init = datetime(year=2005,month=1,day=1),shadow_error = True,num_cols=2,\
              figname = '/Users/carlos/Documents/surface_data_analisis/LastVersion/plot_data/bayes_data.pdf',fontsize=25,colors = 1,save=True)

    
def plot_without_u():
        
    data = data_dataframe('npy_data')

    data['NAP'] = np.nan
    data['CDOM'] = np.nan
    second_run = read_second_run('/Users/carlos/Documents/surface_data_analisis/LastVersion/results_NN',include_uncertainty=False,abr='output')
    data = second_run.merge(data,how='right',on='date')
    data.sort_values(by='date',inplace=True)
    del second_run
    
    lambdas_names = ['412','442','490','510','555']
    lambdas_values = ['412.5','442.5','490','510','555']

    columns = []
    names = []
    labels = []

    for i,lam in enumerate(lambdas_names):
        columns.append(('kd_'+ lam,'kd_output_' + lam))
        names.append('$kd_{'+lambdas_values[i]+'} (m^{-1}$)')
        labels.append(('Buoy data','NN aproach','Buoy data','NN aproach'))




    columns.append(('chla','chla_output'))
    names.append('$Chl-a (mg/m^{3})$')
    labels.append(('Buoy data','NN aproach', 'Buoy data', 'NN aproach'))
    for i,lam in enumerate(lambdas_names):
        if (i == 1) or (i == 2) or (i ==4):
            columns.append(('bbp_'+ lam,'bbp_output_' + lam))
            names.append('$b_{b,p,'+lambdas_values[i]+'} (m^{-1}$)')
            labels.append(('Buoy data','NN aproach','Buoy data','NN aproach'))

    columns.append(('NAP','NAP_output'))
    names.append('$NAP (mg/m^3)$')
    labels.append(('','NN aproach','','NN aproach'))

    columns.append(('CDOM','CDOM_output'))
    names.append('$CDOM (mg/m^3)$')
    labels.append(('','NN aproach','','NN aproach'))


    plot_parallel(data,columns,names,labels,statistics = True,histogram=False,date_init = datetime(year=2005,month=1,day=1),shadow_error = True,num_cols=2,\
              figname = '/Users/carlos/Documents/surface_data_analisis/LastVersion/plot_data/NN_data.pdf',fontsize=25,colors = 3,save=True)


def comparison_alphas():
    data = data_dataframe('/Users/carlos/Documents/surface_data_analisis/LastVersion/npy_data',which = 'all')

    data['NAP'] = np.nan
    data['CDOM'] = np.nan

    second_runs_errors = pd.DataFrame(columns = ['alpha','epsilon_rrs','error_output_mean','error_mean','epsilon_error'])
    alphas = []
    k=0
    for file_ in os.listdir('/Users/carlos/Documents/surface_data_analisis/LastVersion/results_bayes_optimized/alphas'):
        if file_ == 'dates.npy':
            continue
        
        name_ = file_.split('_')
        alpha = '.'.join(name_[2].split('.')[:2])
        if (alpha in alphas):
            pass
        else:
            alphas.append(alpha)
            second_run_i = read_second_run('/Users/carlos/Documents/surface_data_analisis/LastVersion/results_bayes_optimized/alphas',include_uncertainty=True,abr='output',name_index = alpha )
            second_run_i['delta_chla_output'] = (second_run_i['delta_chla_output'])**(1/2)
            epsilon_rrs = data[data.columns[:5]].to_numpy() - second_run_i[second_run_i.columns[:5]].to_numpy()
            epsilon_rrs = np.mean(np.sqrt(np.mean(epsilon_rrs**2,axis=1)))
            error_output_mean = second_run_i[second_run_i.columns[6]].to_numpy().mean()
            error_mean = data[data.columns[22]].to_numpy() - second_run_i[second_run_i.columns[5]].to_numpy()
            error_mean = np.mean(np.sqrt(np.nanmean(error_mean**2)))
            epsilon_error = np.abs(error_mean - error_output_mean)

            second_runs_errors.loc[len(second_runs_errors)] = [float(alpha),epsilon_rrs,error_output_mean,error_mean,epsilon_error]
            k+=1
    #second_runs_errors[second_runs_errors.columns] = np.array(second_runs_errors)

    alpha = '23'
    alphas.append(alpha)
    second_run_i = read_second_run('/Users/carlos/Documents/surface_data_analisis/LastVersion/results_bayes_optimized',include_uncertainty=True,abr='output',name_index = None )
    second_run_i['delta_chla_output'] = (second_run_i['delta_chla_output'])
    epsilon_rrs = data[data.columns[:5]].to_numpy() - second_run_i[second_run_i.columns[:5]].to_numpy()
    epsilon_rrs = np.mean(np.sqrt(np.mean(epsilon_rrs**2,axis=1)))
    error_output_mean = second_run_i[second_run_i.columns[6]].to_numpy().mean()
    error_mean = data[data.columns[22]].to_numpy() - second_run_i[second_run_i.columns[5]].to_numpy()
    error_mean = np.mean(np.sqrt(np.nanmean(error_mean**2)))
    epsilon_error = np.abs(error_mean - error_output_mean)

    second_runs_errors.loc[len(second_runs_errors)] = [float(alpha),epsilon_rrs,error_output_mean,error_mean,epsilon_error]
    second_runs_errors.sort_values(by='alpha',inplace = True)

    normilized_epsilon_rrs = ((second_runs_errors['epsilon_rrs'] -  second_runs_errors['epsilon_rrs'].mean())/second_runs_errors['epsilon_rrs'].std())
    normilized_epsilon_error = ((second_runs_errors['epsilon_error'] -  second_runs_errors['epsilon_error'].mean())/second_runs_errors['epsilon_error'].std())
    normilized_error_chla = ((second_runs_errors['error_mean'] -  second_runs_errors['error_mean'].mean())/second_runs_errors['error_mean'].std())
    

    Loss_function =  normilized_epsilon_rrs + normilized_error_chla + normilized_epsilon_error
    print(second_runs_errors)
    fig,ax = plt.subplots()
    colors = plt.cm.viridis(np.linspace(0,1,17))

    f = scipy.interpolate.interp1d(second_runs_errors['alpha'],normilized_epsilon_rrs,kind='cubic')
    xnew = np.arange(second_runs_errors['alpha'].min(), second_runs_errors['alpha'].max(), 0.5)
    ynew = f(xnew)
    ax.plot(xnew,ynew,'--',color='gray',alpha=0.4)

    f = scipy.interpolate.interp1d(second_runs_errors['alpha'], normilized_epsilon_error,kind='cubic')
    xnew = np.arange(second_runs_errors['alpha'].min(), second_runs_errors['alpha'].max(), 0.5)
    ynew = f(xnew)
    ax.plot(xnew,ynew,'--',color = colors[1],alpha=0.4)

    f = scipy.interpolate.interp1d(second_runs_errors['alpha'], Loss_function,kind='cubic')
    xnew = np.arange(second_runs_errors['alpha'].min(), second_runs_errors['alpha'].max(), 0.5)
    ynew = f(xnew)
    ax.plot(xnew,ynew,'--',color = colors[5], alpha= 0.4)


    f = scipy.interpolate.interp1d(second_runs_errors['alpha'], normilized_error_chla,kind='cubic')
    xnew = np.arange(second_runs_errors['alpha'].min(), second_runs_errors['alpha'].max(), 0.5)
    ynew = f(xnew)
    ax.plot(xnew,ynew,'--',color = colors[11], alpha= 0.4)

    s = 15
    ax.scatter(second_runs_errors['alpha'],normilized_epsilon_rrs,label='$\epsilon_{R_{RS}} = \overline{ RMSD(R_{RS}^{OBS},R_{RS}^{MOD})}$',c='black',s=s)
    ax.scatter(second_runs_errors['alpha'],normilized_error_chla,label='$\epsilon_{chla} = \overline{ RMSD(chla^{OBS} - chla^{MOD})}$',c = colors[11],marker='4',s=s)
    ax.scatter(second_runs_errors['alpha'],normilized_epsilon_error,label='$\epsilon_{\delta_{chla}} = \overline{ MEAN(|RMSD(chla^{OBS} - chla^{MOD}) - MEAN(\delta_{chla})|)}$',c = colors[1],marker='x',s=s)
    ax.scatter(second_runs_errors['alpha'],Loss_function,label='$\mathbf{L} =\overline{ \epsilon_{R_{RS}} + \epsilon_{chla} + \epsilon_{\delta_{chla}}}$',c = colors[5],marker='^',s=s)
    ax.axvline(23,linestyle='--',color='red')
    ax.text(24,2,'$\\text{argmin}_{\\alpha}(\mathbf{L}$)',color=colors[5])


    ax.legend(fontsize="10.5",bbox_to_anchor=(0.1,0.65),loc='lower left',shadow=False)

    ax.set_xlabel('A-priori covariance for chl-a, NAP and CDOM ($\\alpha$)')
    ax.set_ylabel('Normilized errors')
    
    fig.tight_layout()
    plt.show()
            


                

        

            
    

    
plot_without_u()
plot_with_u()
#comparison_alphas()
