"""
Inversion problemas a NN.

As part of the National Institute of Oceanography, and Applied Geophysics, I'm working on an inversion problem. A detailed description can be found at
https://github.com/carlossoto362/firstModelOGS.

"""
import matplotlib.pyplot as plt
import math
import numpy as np
import torch
from torch.distributions.multivariate_normal import MultivariateNormal
from torch import nn
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader,random_split

from ray import tune
from ray import train
from ray.train import Checkpoint, get_checkpoint
from ray.tune.schedulers import ASHAScheduler
import ray.cloudpickle as pickle
import tempfile
from pathlib import Path
from functools import partial
from ray.tune.schedulers.hb_bohb import HyperBandForBOHB
from ray.tune.search.bohb import TuneBOHB
import ConfigSpace as CS

from datetime import datetime, timedelta
import pandas as pd
import os
import scipy
from scipy import stats
import time
import torch.distributed as dist
import sys

import warnings
import Forward_module as fm
import read_data_module as rdm
from CVAE_model_part_one import NN_first_layer


class NN_second_layer_mean(nn.Module):

    def __init__(self,precision = torch.float32,output_layer_size_mean=3,number_hiden_layers_mean = 1,\
                 dim_hiden_layers_mean = 20,alpha_mean=1,dim_last_hiden_layer_mean = 1,output_layer_size_cov=9,number_hiden_layers_cov = 1,\
                 dim_hiden_layers_cov = 20,alpha_cov=1,dim_last_hiden_layer_cov = 1,x_mul=None,x_add=None,y_mul=None,y_add=None,constant = None,model_dir=None):
        super().__init__()

        
        self.flatten = nn.Flatten()

        def read_NN_first_layer(model_dir='./VAE_model'):
            """
            read rirst layer trained with in situ data. This layer is made to facilitate the learning of
            the latent such that they are close to in-situ observations.
            """
            model_first_layer = NN_first_layer(precision = torch.float32,input_layer_size=17,output_layer_size=9,\
                           number_hiden_layers = 1,dim_hiden_layers = 20,dim_last_hiden_layer = 22,alpha=0.8978238833058).to(my_device)
            model_first_layer.load_state_dict(torch.load(model_dir+'/model_first_part.pt'))
            model_first_layer.eval()
            for param in model_first_layer.parameters():
                param.requires_grad = False
            return model_first_layer
        self.first_layer = read_NN_first_layer(model_dir)
        

        linear_celu_stack_mean = []
        input_size = 9
        if hasattr(dim_hiden_layers_mean, '__iter__'):
            output_size = dim_hiden_layers_mean[0]
        else:
            output_size = dim_hiden_layers_mean
            
        for hl in range(number_hiden_layers_mean):
            if hl != (number_hiden_layers_mean - 1):
                linear_celu_stack_mean += [nn.Linear(input_size,output_size),nn.CELU(alpha=alpha_mean)]
                input_size = output_size
                if hasattr(dim_hiden_layers_mean, '__iter__'):
                    output_size = dim_hiden_layers_mean[hl+1]
                else:
                    output_size = dim_hiden_layers_mean
            else:
                linear_celu_stack_mean += [nn.Linear(input_size,dim_last_hiden_layer_mean),nn.CELU(alpha=alpha_mean)]  
        linear_celu_stack_mean += [nn.Linear(dim_last_hiden_layer_mean,output_layer_size_mean),nn.CELU(alpha=alpha_mean)]
        self.linear_celu_stack_mean = nn.Sequential( *linear_celu_stack_mean  )

        linear_celu_stack_cov = []
        input_size = 9
        if hasattr(dim_hiden_layers_cov, '__iter__'):
            output_size = dim_hiden_layers_cov[0]
        else:
            output_size = dim_hiden_layers_cov
            
        for hl in range(number_hiden_layers_cov):
            if hl != (number_hiden_layers_cov - 1):
                linear_celu_stack_cov += [nn.Linear(input_size,output_size),nn.CELU(alpha=alpha_cov)]
                input_size = output_size
                if hasattr(dim_hiden_layers_cov, '__iter__'):
                    output_size = dim_hiden_layers_cov[hl+1]
                else:
                    output_size = dim_hiden_layers_cov
            else:
                linear_celu_stack_cov += [nn.Linear(input_size,dim_last_hiden_layer_cov),nn.CELU(alpha=alpha_cov)]  
        linear_celu_stack_cov += [nn.Linear(dim_last_hiden_layer_cov,output_layer_size_cov),nn.CELU(alpha=alpha_cov)]
        self.linear_celu_stack_cov = nn.Sequential( *linear_celu_stack_cov  )

        self.x_mul = torch.tensor(x_mul).to(precision)
        self.y_mul = torch.tensor(y_mul).to(precision)
        self.x_add = torch.tensor(x_add).to(precision)
        self.y_add = torch.tensor(y_add).to(precision)

        self.Forward_Model = fm.Forward_Model(learning_chla = False, learning_perturbation_factors = True)
        self.constant = constant

    def rearange_RRS(self,x):
        lambdas = torch.tensor([412.5,442.5,490.,510.,555.])
        x_ = x*self.x_mul + self.x_add
        output = torch.empty((len(x),5,5))
        output[:,:,0] = x_[:,0,5:10]
        output[:,:,1] = x_[:,0,10:15]
        output[:,:,2] = lambdas
        output[:,:,3] = x_[:,:,15]
        output[:,:,4] = x_[:,:,16]
        return output



    def forward(self, image):
        x = self.first_layer(image)
        mu_z = self.linear_celu_stack_mean(x).flatten(1) + torch.column_stack((x[:,0,0],torch.zeros(x.shape[0],2)))
        cov_z = self.linear_celu_stack_cov(x).reshape((x.shape[0],3,3))

        epsilon = MultivariateNormal(torch.zeros(3),scale_tril=torch.eye(3)).sample(sample_shape=torch.Size([x.shape[0],1]))
        z_hat = mu_z + torch.transpose(cov_z@torch.transpose(epsilon,dim0=1,dim1=2),dim0=1,dim1=2).flatten(1) #transforming to do matmul in the correct dimention, then going back to normal.

        z_hat = (z_hat * self.y_mul[0] + self.y_add[0]).unsqueeze(1)
        image = self.rearange_RRS(image)
        
        rrs_ = self.Forward_Model(image,parameters = z_hat,constant = self.constant)
        rrs_ = (rrs_ - self.x_add[:5])/self.x_mul[:5]
 
        kd_ = fm.kd(9.,image[:,:,0],image[:,:,1],image[:,:,2],image[:,:,3],image[:,:,4],torch.exp(z_hat[:,:,0]),torch.exp(z_hat[:,:,1]),torch.exp(z_hat[:,:,2]),self.Forward_Model.perturbation_factors,constant = self.constant)
        kd_ = (kd_  - self.y_add[1:6])/self.y_mul[1:6]

        bbp_ = fm.bbp(image[:,:,0],image[:,:,1],image[:,:,2],image[:,:,3],image[:,:,4],torch.exp(z_hat[:,:,0]),torch.exp(z_hat[:,:,1]),torch.exp(z_hat[:,:,2]),self.Forward_Model.perturbation_factors,constant = self.constant)[:,[1,2,4]]
        bbp_ = (bbp_ - self.y_add[6:])/self.y_mul[6:9]

        mu_z = mu_z * self.y_mul[0] + self.y_add[1]
        
        cov_z = torch.diag(self.y_mul[0].expand(3)).T @ cov_z @ torch.diag(self.y_mul[0].expand(3))

        return z_hat[:,0,:],cov_z,mu_z,kd_,bbp_,rrs_

if __name__ == '__main__':
    data_path = '/Users/carlos/Documents/OGS_one_d_model/npy_data'
    my_device = 'cpu'
    constant = rdm.read_constants(file1='cte_lambda.csv',file2='cst.csv',my_device = my_device)
    data = rdm.customTensorData(data_path=data_path,which='train',per_day = False,randomice=True,one_dimensional = True,seed = 1853,normilized_NN='scaling')
    
    model = NN_second_layer_mean(output_layer_size_mean=3,number_hiden_layers_mean = 1,\
                                 dim_hiden_layers_mean = 20,alpha_mean=1,dim_last_hiden_layer_mean = 1,output_layer_size_cov=9,number_hiden_layers_cov = 1,\
                                 dim_hiden_layers_cov = 20,alpha_cov=1,dim_last_hiden_layer_cov = 1,x_mul=(data.x_max - data.x_min),x_add=data.x_min,\
                                 y_mul=(data.y_max - data.y_min),y_add=data.y_min,constant = constant,model_dir = './VAE_model')
    dataloader = DataLoader(data, batch_size=len(data.dates), shuffle=True)
    X,Y = next(iter(dataloader))
    z_hat,cov_z,mu_z,kd_,bbp_,rrs_ = model(X)
    print(list(iter(model.first_layer.parameters()))[0].requires_grad)
